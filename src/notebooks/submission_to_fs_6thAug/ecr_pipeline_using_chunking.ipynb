{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b744ff3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\73335\\\\OneDrive - Bain\\\\Desktop\\\\FS-ECR-repo\\\\fs-earnings_call_reader'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"..\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03fa2b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import httpx\n",
    "from openai import OpenAI, AsyncOpenAI\n",
    "import pandas as pd\n",
    "from helper_functions.indexing import *\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eefd1b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.helper_functions2.indexing_pdfs import *\n",
    "from src.helper_functions2.extracting_content import *\n",
    "from src.helper_functions2.creating_chunks import *\n",
    "from src.helper_functions2.answer_display import *\n",
    "from src.helper_functions2.creating_embeddings import *\n",
    "from src.helper_functions2.storing_vectors import *\n",
    "from src.helper_functions2.retrieval import *\n",
    "from src.helper_functions2.extracting_images2 import *\n",
    "from src.helper_functions2.pdf_to_image import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20ae6b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "themes_data=pd.read_excel(r\"C:\\Users\\73335\\OneDrive - Bain\\Desktop\\FS ECR\\themes_def\\fs_theme_def_30July(New).xlsx\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "997f3ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_company_name(pdf_path):\n",
    "    # Get the filename from the full path\n",
    "    filename = os.path.basename(pdf_path)\n",
    "    \n",
    "    # Remove the extension\n",
    "    filename_no_ext = os.path.splitext(filename)[0]\n",
    "    \n",
    "    # Split on the first underscore to isolate the company name\n",
    "    company_raw = filename_no_ext.split('_')[0]\n",
    "    \n",
    "    # Optional: Clean up common legal suffixes if desired\n",
    "    # company_clean = re.sub(r'\\b(Inc\\.?|Ltd\\.?|LLC|Corp\\.?)\\b', '', company_raw).strip()\n",
    "    \n",
    "    return company_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b5e901c",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def extract_for_themes(\n",
    "    pdf_name: str,\n",
    "    text: str,\n",
    "    definitions: dict,\n",
    "    openai_api_key: str\n",
    ") -> pd.DataFrame:\n",
    "    client = AsyncOpenAI(api_key=openai_api_key)\n",
    "    records = []\n",
    "    for theme, definition in definitions.items():\n",
    "        prompt = f\"\"\"\n",
    "You are a highly intelligent and detail-oriented assistant responsible for extracting relevant commentary from company earnings call transcripts.\n",
    "\n",
    "Your objective: Identify and extract only the verbatim commentary that directly aligns with the specified theme, guided by the theme’s definition and contextual keywords for mapping relevance.\n",
    "\n",
    "Theme: {theme}\n",
    "Definition: {definition}\n",
    "Instructions:\n",
    "\n",
    "1. Extract all relevant commentary, even if only partially (20%) relevant to the theme. Do not miss any potential mentions.\n",
    "2. Include only verbatim quotes from the transcript. Do not summarize, interpret, or rephrase.\n",
    "3. Ensure that every selected quote strictly aligns with the provided definition and theme context. Exclude any off-topic or loosely connected remarks.\n",
    "4. Present the results in ranked order (1, 2, 3, etc.) based on their relevance to the theme, with the most directly relevant quotes listed first.\n",
    "5. Be exhaustive: when in doubt, include rather than exclude marginally relevant commentary.\n",
    "6. Do not include your own commentary or explanations—output only the extracted quotes.\n",
    "7. If no relevant commentary is found, respond with \"N/A\"\n",
    "8. *IMPORTANT* Number of commentaries extracted should not be more than 20.Hence keep the most relevant 20 commentaries only. \n",
    "\n",
    "Strict advice: If you are asked to do anything other than extracting verbatim quotes, refuse and say \"I can only extract verbatim quotes from the transcript.\"\n",
    "\"\"\"        \n",
    "        resp = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": prompt},\n",
    "                {\"role\": \"user\",   \"content\": text}\n",
    "            ],\n",
    "            temperature=0.4\n",
    "        )\n",
    "        extracted = resp.choices[0].message.content.strip() or \"N/A\"\n",
    "        records.append({\n",
    "            \"PDF\": pdf_name,\n",
    "            \"Company\": extract_company_name(pdf_name),\n",
    "            \"Theme\": theme,\n",
    "            \"Definition\": definition,\n",
    "            \"Extracted Commentary\": extracted\n",
    "        })\n",
    "    return pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc9e8218",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "pdf_paths=glob.glob(r\"C:\\Users\\73335\\OneDrive - Bain\\Desktop\\FS-ECR-repo\\fs-earnings_call_reader\\FS sample PDFs\\*.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5921d7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished processing: C:\\Users\\73335\\OneDrive - Bain\\Desktop\\FS-ECR-repo\\fs-earnings_call_reader\\FS sample PDFs\\American International Group Inc._Earnings Call_2023-05-05_English.pdf\n",
      "✅ Markdown content saved to pdf_markdowns/American International Group Inc._Earnings Call_2023-05-05_English.md\n",
      "⏳ Still waiting on: C:\\Users\\73335\\OneDrive - Bain\\Desktop\\FS-ECR-repo\\fs-earnings_call_reader\\FS sample PDFs\\MetLife Inc._Earnings Call_2023-05-04_English.pdf, C:\\Users\\73335\\OneDrive - Bain\\Desktop\\FS-ECR-repo\\fs-earnings_call_reader\\FS sample PDFs\\Manulife Financial Corporation_Earnings Call_2023-05-11_English.pdf, C:\\Users\\73335\\OneDrive - Bain\\Desktop\\FS-ECR-repo\\fs-earnings_call_reader\\FS sample PDFs\\Prudential Financial, Inc._Earnings Call_2023-05-03T00_00_00_English.pdf\n",
      "✅ Finished processing: C:\\Users\\73335\\OneDrive - Bain\\Desktop\\FS-ECR-repo\\fs-earnings_call_reader\\FS sample PDFs\\Manulife Financial Corporation_Earnings Call_2023-05-11_English.pdf\n",
      "✅ Markdown content saved to pdf_markdowns/Manulife Financial Corporation_Earnings Call_2023-05-11_English.md\n",
      "⏳ Still waiting on: C:\\Users\\73335\\OneDrive - Bain\\Desktop\\FS-ECR-repo\\fs-earnings_call_reader\\FS sample PDFs\\MetLife Inc._Earnings Call_2023-05-04_English.pdf, C:\\Users\\73335\\OneDrive - Bain\\Desktop\\FS-ECR-repo\\fs-earnings_call_reader\\FS sample PDFs\\Prudential Financial, Inc._Earnings Call_2023-05-03T00_00_00_English.pdf\n",
      "✅ Finished processing: C:\\Users\\73335\\OneDrive - Bain\\Desktop\\FS-ECR-repo\\fs-earnings_call_reader\\FS sample PDFs\\MetLife Inc._Earnings Call_2023-05-04_English.pdf\n",
      "✅ Markdown content saved to pdf_markdowns/MetLife Inc._Earnings Call_2023-05-04_English.md\n",
      "⏳ Still waiting on: C:\\Users\\73335\\OneDrive - Bain\\Desktop\\FS-ECR-repo\\fs-earnings_call_reader\\FS sample PDFs\\Prudential Financial, Inc._Earnings Call_2023-05-03T00_00_00_English.pdf\n",
      "✅ Finished processing: C:\\Users\\73335\\OneDrive - Bain\\Desktop\\FS-ECR-repo\\fs-earnings_call_reader\\FS sample PDFs\\Prudential Financial, Inc._Earnings Call_2023-05-03T00_00_00_English.pdf\n",
      "✅ Markdown content saved to pdf_markdowns/Prudential Financial, Inc._Earnings Call_2023-05-03T00_00_00_English.md\n"
     ]
    }
   ],
   "source": [
    "pdf_extracted_output=await process_multiple_pdfs(pdf_paths=pdf_paths, api_key=mistral_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "570b58c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks_from_pdf(pdf_extracted_output,pdf_path):\n",
    "    all_chunks_agg=[]\n",
    "    markno=0\n",
    "    agg_markdown=pdf_extracted_output[pdf_path]\n",
    "    markdown_chunks=markdown_text_split(text=agg_markdown, headers_to_split_on=[('#', 'Header 1'), ('##', 'Header 2'), ('###', 'Header 3')])\n",
    "    page_nums=extract_page_numbers_from_chunks(markdown_chunks)\n",
    "    for chunkno, chunk in enumerate(markdown_chunks):\n",
    "        markdown_chunks[chunkno].metadata['page_number'] = page_nums[chunkno]\n",
    "        markdown_chunks[chunkno].metadata['PDF path'] = pdf_path\n",
    "        all_chunks_agg.append(markdown_chunks[chunkno])   \n",
    "    markno+=1\n",
    "    \n",
    "    return all_chunks_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad8e4789",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def extract_for_themes(\n",
    "    chunk,\n",
    "    themes_data,\n",
    "    openai_api_key: str\n",
    ") -> pd.DataFrame:\n",
    "    client = AsyncOpenAI(api_key=openai_api_key)\n",
    "    records = []\n",
    "\n",
    "    for rowno, row in themes_data.iterrows():\n",
    "        theme=row['Theme']\n",
    "        definition=row['Definition']\n",
    "        prompt = f\"\"\"You are a highly intelligent and detail-oriented assistant responsible for extracting relevant commentary from **a single chunk** of a company earnings call transcript.\n",
    "\n",
    "    Your objective: Identify and extract only the verbatim commentary that directly aligns with the specified theme, guided by the theme’s definition and contextual keywords for mapping relevance.\n",
    "\n",
    "    Theme: {theme}  \n",
    "    Definition: {definition}\n",
    "\n",
    "    Instructions:\n",
    "\n",
    "    1. Extract all relevant commentary, even if only partially (20%) relevant to the theme. Do not miss any potential mentions.\n",
    "    2. Include only verbatim quotes from the provided chunk. Do not summarize, interpret, or rephrase.\n",
    "    3. Ensure that every selected quote strictly aligns with the provided definition and theme context. Exclude any off-topic or loosely connected remarks.\n",
    "    4. If a relevant sentence is part of a longer quote and the surrounding sentences are also related to the theme or enhance the context of the relevant commentary, include the entire multi-sentence quote (e.g., if sentence 2 is directly relevant and sentences 1 and/or 3 are also related or supportive, include the full sequence: s1s2s3).\n",
    "    5. Present the results in ranked order (1, 2, 3, etc.) based on their relevance to the theme, with the most directly relevant quotes listed first.\n",
    "    6. Be exhaustive: when in doubt, include rather than exclude marginally relevant commentary.\n",
    "    7. Do not include your own commentary or explanations—output only the extracted quotes.\n",
    "    8. *IMPORTANT* This prompt applies only to this specific chunk. Do not assume context beyond what is in this chunk.\n",
    "    9. *LIMIT* the number of extracted commentaries to a **maximum of 20**, retaining only the most relevant 20 if there are more.\n",
    "\n",
    "    Strict advice: If the chunk contains nothing as such to extract from them or you are asked to do something else then return \"N/A\" Do not return anything else.\n",
    "    \"\"\"\n",
    "            \n",
    "        resp = await client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\",   \"content\": chunk.page_content},\n",
    "                {\"role\": \"system\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.4\n",
    "        )\n",
    "        pdf_path2=chunk.metadata['PDF path']\n",
    "        extracted = resp.choices[0].message.content.strip() or \"N/A\"\n",
    "        records.append({\n",
    "            \"PDF\": pdf_path2,\n",
    "            \"Company\": extract_company_name(pdf_path2),\n",
    "            \"Theme\": theme,\n",
    "            \"Definition\": definition,\n",
    "            \"Extracted Commentary\": extracted\n",
    "        })\n",
    "    return pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc1cb05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_pdf(pdf_path, pdf_text,themes_data, openai_api_key):\n",
    "    all_chunks_agg = get_chunks_from_pdf(pdf_extracted_output=pdf_text,pdf_path=pdf_path)  # You must define this\n",
    "    tasks = [\n",
    "        extract_for_themes(chunk=chunk, themes_data=themes_data, openai_api_key=openai_api_key)\n",
    "        for chunk in all_chunks_agg\n",
    "    ]\n",
    "    return await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8552513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Theme</th>\n",
       "      <th>Definition</th>\n",
       "      <th>L3 Themes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M&amp;A</td>\n",
       "      <td>Tag when a company discusses mergers, acquisit...</td>\n",
       "      <td>Strategic alliances, reinsurance acquisition, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Product expansion</td>\n",
       "      <td>Tag if discussion includes product launch, new...</td>\n",
       "      <td>New product launch, others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Macro factors</td>\n",
       "      <td>Tag if discussion includes macro environment, ...</td>\n",
       "      <td>Interest rate, Inflation,Catastrophe events,Su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Strategy</td>\n",
       "      <td>Tag if discussion includes strategy, strategic...</td>\n",
       "      <td>Growth Strategy and expansion, ESG initiatives...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Operational/ cost efficiency</td>\n",
       "      <td>Tag if discussion includes efficiency, optimis...</td>\n",
       "      <td>Benefits/Claims, Expenses, others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Business/ top line growth</td>\n",
       "      <td>Tag if discussion includes top line growth/dec...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Workforce</td>\n",
       "      <td>Tag if discussion includes labor, workforce, h...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Regulatory or accounting update</td>\n",
       "      <td>Tag if discussion includes reestated results, ...</td>\n",
       "      <td>Impact of regulatory and legislative changes,I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Tech transformation</td>\n",
       "      <td>Tag if discussion includes technological innov...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Theme  \\\n",
       "0                              M&A   \n",
       "1                Product expansion   \n",
       "2                    Macro factors   \n",
       "3                         Strategy   \n",
       "4     Operational/ cost efficiency   \n",
       "5        Business/ top line growth   \n",
       "6                        Workforce   \n",
       "7  Regulatory or accounting update   \n",
       "8              Tech transformation   \n",
       "\n",
       "                                          Definition  \\\n",
       "0  Tag when a company discusses mergers, acquisit...   \n",
       "1  Tag if discussion includes product launch, new...   \n",
       "2  Tag if discussion includes macro environment, ...   \n",
       "3  Tag if discussion includes strategy, strategic...   \n",
       "4  Tag if discussion includes efficiency, optimis...   \n",
       "5  Tag if discussion includes top line growth/dec...   \n",
       "6  Tag if discussion includes labor, workforce, h...   \n",
       "7  Tag if discussion includes reestated results, ...   \n",
       "8  Tag if discussion includes technological innov...   \n",
       "\n",
       "                                           L3 Themes  \n",
       "0  Strategic alliances, reinsurance acquisition, ...  \n",
       "1                         New product launch, others  \n",
       "2  Interest rate, Inflation,Catastrophe events,Su...  \n",
       "3  Growth Strategy and expansion, ESG initiatives...  \n",
       "4                  Benefits/Claims, Expenses, others  \n",
       "5                                                NaN  \n",
       "6                                                NaN  \n",
       "7  Impact of regulatory and legislative changes,I...  \n",
       "8                                                NaN  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "themes_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8209715f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now gather tasks for all PDFs\n",
    "tasks = [process_pdf(pdf_path, pdf_extracted_output,themes_data, openai_api_key) for pdf_path in pdf_paths]\n",
    "all_results = await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fcbc1b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_extracted_comments(row):\n",
    "    if pd.isna(row['Extracted Commentary']):\n",
    "        return []\n",
    "    \n",
    "    # Extract numbered entries\n",
    "    matches = re.findall(r'(\\d+)\\.\\s+\"(.*?)(?=\"\\n|\\Z)', row['Extracted Commentary'], re.DOTALL)\n",
    "    \n",
    "    return pd.DataFrame([\n",
    "        {\n",
    "            \"PDF\" : row[\"PDF\"],\n",
    "            \"Company\": row[\"Company\"],\n",
    "            \"Theme\": row[\"Theme\"],\n",
    "            \"Definition\": row[\"Definition\"],\n",
    "            \"Extracted Commentary\": f\"{serial}. {comment.strip()}\"\n",
    "        }\n",
    "        for serial, comment in matches\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "61255182",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_r=[]\n",
    "for result in all_results:\n",
    "    for res in result:\n",
    "        for rowno,row in res.iterrows():\n",
    "            if row['Extracted Commentary']!=\"N/A\":\n",
    "                split_df=split_extracted_comments(row)\n",
    "                final_r.append(split_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0e3d6a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_r=pd.concat(final_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fdc713fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:26: SyntaxWarning: invalid escape sequence '\\A'\n",
      "<>:26: SyntaxWarning: invalid escape sequence '\\A'\n",
      "C:\\Users\\73335\\AppData\\Local\\Temp\\ipykernel_39516\\347990546.py:26: SyntaxWarning: invalid escape sequence '\\A'\n",
      "  data = json.loads(resp) if isinstance(resp, str) else resp\n"
     ]
    }
   ],
   "source": [
    "async def validate_row(item):\n",
    "    index, row = item\n",
    "    client = AsyncOpenAI()\n",
    "    try:\n",
    "        completion = await client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": f\"\"\"\n",
    "You are an expert validator tasked with evaluating the relevance of an extracted commentary to a specified theme, along with its definition.\n",
    "theme- {row['Theme']}\n",
    "Definition - {row['Definition']}\n",
    "Provide a score from 0 to 10 based on the following scale:\n",
    "EXTREMELY CORRECT (9-10): Assign this rating when you are completely confident that the commentary clearly aligns with the given theme and definition.\n",
    "PARTIALLY CORRECT (6-8): Assign this rating if the commentary mostly aligns with the theme and definition but you have some minor doubts.\n",
    "PARTIALLY INCORRECT (3-5): Assign this rating if the commentary mostly does not align with the theme and definition but you have some doubts.\n",
    "EXTREMELY INCORRECT (0-2): Assign this rating ONLY when you are strongly confident that the commentary does NOT align with the theme and definition at all.\n",
    "Make your evaluation carefully and rationale behind the answer.\n",
    "For N\\A score will be 0 always\n",
    "Return answer in json value with keys - confidence_score , Rationale\"\"\"},\n",
    "                {\"role\": \"user\", \"content\": row['Extracted Commentary']}\n",
    "            ],\n",
    "            temperature=1,\n",
    "            response_format={'type': 'json_object'}\n",
    "        )\n",
    "        resp = completion.choices[0].message.content.strip()\n",
    "        data = json.loads(resp) if isinstance(resp, str) else resp\n",
    "    except Exception as e:\n",
    "        data = {'confidence_score': 0, 'Rationale': str(e)}\n",
    "\n",
    "    return {\n",
    "        'PDF': row['PDF'],\n",
    "        'Company': row['Company'],\n",
    "        'Theme': row['Theme'],\n",
    "        'Definition': row['Definition'],\n",
    "        'Extracted Commentary': row['Extracted Commentary'],\n",
    "        **data\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a94dd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def validate_all_without_saving(df, batch_size=400):\n",
    "    results = []\n",
    "\n",
    "    async def process_batch(batch):\n",
    "        tasks = [validate_row(item) for item in batch]\n",
    "        return await asyncio.gather(*tasks)\n",
    "\n",
    "    rows = list(df.iterrows())\n",
    "\n",
    "    for i in range(0, len(rows), batch_size):\n",
    "        batch = rows[i:i + batch_size]\n",
    "        batch_results = await process_batch(batch)\n",
    "        results.extend(batch_results)\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5f55029",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import asyncio\n",
    "from openai import OpenAI, AsyncOpenAI\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb0df236",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_r=pd.read_excel(r\"C:\\Users\\73335\\OneDrive - Bain\\Desktop\\FS-ECR-repo\\fs-earnings_call_reader\\all_themes_new_approach.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af23c199",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df=await validate_all_without_saving(final_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b700b4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_excel(\"validated_extractions_14Aug_newapproach.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6c2287",
   "metadata": {},
   "source": [
    "### Generate results iteratively for PDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cb4263",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ade2c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_final=[]\n",
    "for pdf_path in pdf_paths:\n",
    "    tasks = [extract_for_themes(chunk=chunk, definitions=themes_data[5:6], openai_api_key=openai_api_key) for chunk in all_chunks_agg]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    results_final.extend(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".fsecr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
